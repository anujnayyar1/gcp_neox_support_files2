{"vocab-file":"./20B_checkpoints/20B_tokenizer.json","save":"./20B_checkpoints","load":"./20B_checkpoints","data-path":"./data/pile_20B_tokenizer/pile_20B_tokenizer_text_document","pipe-parallel-size":4,"model-parallel-size":2,"num-layers":44,"hidden-size":6144,"num-attention-heads":64,"seq-length":2048,"max-position-embeddings":2048,"norm":"layernorm","pos-emb":"rotary","rotary_pct":0.25,"no-weight-tying":true,"gpt_j_residual":true,"output_layer_parallelism":"column","scaled-upper-triang-masked-softmax-fusion":true,"bias-gelu-fusion":true,"init_method":"small_init","output_layer_init_method":"wang_init","optimizer":{"type":"Adam","params":{"lr":0.97e-4,"betas":[0.9,0.95],"eps":1.0e-8,}},"min_lr":0.97e-5,"zero_optimization":{"stage":1,"allgather_partitions":True,"allgather_bucket_size":1260000000,"overlap_comm":True,"reduce_scatter":True,"reduce_bucket_size":1260000000,"contiguous_gradients":True,"cpu_offload":False},"train_micro_batch_size_per_gpu":4,"gradient_accumulation_steps":32,"data-impl":"mmap","split":"995,4,1","checkpoint-activations":true,"checkpoint-num-layers":1,"partition-activations":false,"synchronize-each-layer":true,"gradient_clipping":1.0,"weight-decay":0.01,"hidden-dropout":0,"attention-dropout":0,"fp16":{"fp16":true,"enabled":true,"loss_scale":0,"loss_scale_window":1000,"initial_scale_power":12,"hysteresis":2,"min_loss_scale":1},"train-iters":150000,"lr-decay-iters":150000,"distributed-backend":"nccl","lr-decay-style":"cosine","warmup":0.01,"save-interval":500,"eval-interval":1000,"eval-iters":10,"log-interval":2,"steps_per_print":2,"wall_clock_breakdown":false,"tokenizer_type":"HFTokenizer","tensorboard-dir":"./tensorboard","log-dir":"./logs","config_files":{"config.yml":"attention-dropout: 0\nbias-gelu-fusion: true\ncheckpoint-activations: true\ncheckpoint-num-layers: 1\ndata-impl: mmap\ndata-path: /mnt/ssd-1/data/pile_20B_tokenizer/pile_20B_tokenizer_text_document\ndistributed-backend: nccl\neval-interval: 1000\neval-iters: 10\nfp16:\n  enabled: true\n  fp16: true\n  hysteresis: 2\n  initial_scale_power: 12\n  loss_scale: 0\n  loss_scale_window: 1000\n  min_loss_scale: 1\ngpt_j_residual: true\ngradient_accumulation_steps: 32\ngradient_clipping: 1.0\nhidden-dropout: 0\nhidden-size: 6144\ninit_method: small_init\nload: ./20B_checkpoints\nlog-dir: /mnt/ssd-1/logs\nlog-interval: 2\nlr-decay-iters: 150000\nlr-decay-style: cosine\nmake_vocab_size_divisible_by: 256\nmax-position-embeddings: 2048\nmin_lr: 9.7e-06\nmodel-parallel-size: 1\nno-weight-tying: true\nno_load_rng: true\nnorm: layernorm\nnum-attention-heads: 64\nnum-layers: 44\noptimizer:\n  params:\n    betas:\n    - 0.9\n    - 0.95\n    eps: 1.0e-08\n    lr: 9.7e-05\n  type: Adam\noutput_layer_init_method: wang_init\noutput_layer_parallelism: column\npartition-activations: false\npipe-parallel-size: 1\npos-emb: rotary\nrotary_pct: 0.25\nsave: checkpoints_merged\nsave-interval: 500\nscaled-upper-triang-masked-softmax-fusion: true\nseq-length: 2048\nsplit: 995,4,1\nsteps_per_print: 2\nsynchronize-each-layer: true\ntensorboard-dir: /mnt/ssd-1/tensorboard\ntokenizer_type: HFTokenizer\ntrain-iters: 150000\ntrain_micro_batch_size_per_gpu: 4\nvocab-file: /root/gpt-neox/checkpoints/20B_tokenizer.json\nwall_clock_breakdown: false\nwandb_project: gpt-thicc\nwandb_team: eleutherai\nwarmup: 0.01\nweight-decay: 0.01\nzero_optimization:\n  allgather_bucket_size: 1260000000\n  allgather_partitions: true\n  contiguous_gradients: true\n  cpu_offload: false\n  overlap_comm: true\n  reduce_bucket_size: 1260000000\n  reduce_scatter: true\n  stage: 0\n","text_generation.yml":"# Parameters used for text generation\n# Make sure `load` is specified somewhere else\n{\n  # Text gen type: `input-file`, `unconditional` or `interactive`\n  \"text-gen-type\": \"prompt\",\n\n  # Params for all\n  \"maximum_tokens\": 10,\n  \"temperature\": 1.0,\n  \"top_p\": 0.0,\n  \"top_k\": 0,\n  \"recompute\": false,\n\n  # `unconditional`: samples\n  \"num-samples\": 10,\n\n  # input/output file\n  \"sample-input-file\": \"sample_input.txt\",\n  \"sample-output-file\": \"sample_output.txt\",\n}\n"},"load":"./20B_checkpoints","save_interval":500,"no_load_rng":true,"batch_size":4,"train_iters":150000,"eval_iters":10,"split":"995,4,1","vocab_file":"/root/gpt-neox/20B_checkpoints/20B_tokenizer.json","attention_dropout":0,"hidden_dropout":0,"checkpoint_activations":true,"synchronize_each_layer":true,"gas":32,"clip_grad":1.0,"dynamic_loss_scale":true,"pipe_parallel_size":1,"is_pipe_parallel":true,"wandb_group":"kNEzESq2hBFuC7yi64sRAV_2c47g9im","wandb_team":"eleutherai","wandb_project":"gpt-thicc","log_dir":"/mnt/ssd-1/logs","tensorboard_dir":"/mnt/ssd-1/tensorboard","log_interval":2,"text_gen_type":"prompt","temperature":1.0,"maximum_tokens":10,"sample_input_file":"sample_input.txt","sample_output_file":"sample_output.txt","num_samples":10,"user_script":"generate.py"}
