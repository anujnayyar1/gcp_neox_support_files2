{"world_size": 2, "pipe_parallel_size":1, "model-parallel-size": 2, "train_micro_batch_size_per_gpu":4,"optimizer":{"params":{"betas":[0.9,0.95],"eps":1e-08,"lr":9.7e-05},"type":"Adam"},"fp16":{"enabled":true,"fp16":true,"hysteresis":2,"initial_scale_power":12,"loss_scale":0,"loss_scale_window":1000,"min_loss_scale":1},"gradient_clipping":1.0,"zero_optimization":{"allgather_bucket_size":1260000000,"allgather_partitions":true,"contiguous_gradients":true,"cpu_offload":false,"overlap_comm":true,"reduce_bucket_size":1260000000,"reduce_scatter":true,"stage":0},"steps_per_print":2,"precision":"fp16","num_layers":44,"hidden_size":6144,"num_attention_heads":64,"seq_length":2048,"max_position_embeddings":2048,"pos_emb":"rotary","no_weight_tying":true,"attention_config":["global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global"],"sparsity_config":{},"make_vocab_size_divisible_by":256,"scaled_upper_triang_masked_softmax_fusion":true,"bias_gelu_fusion":true,"rotary_pct":0.25,"init_method":"small_init","output_layer_init_method":"wang_init","gpt_j_residual":true,"output_layer_parallelism":"column","lr_decay_style":"cosine","lr_decay_iters":150000,"min_lr":9.7e-06,"optimizer_type":"Adam","zero_stage":0,"zero_reduce_scatter":true,"zero_contiguous_gradients":true,"zero_reduce_bucket_size":1260000000,"zero_allgather_bucket_size":1260000000,"lr":9.7e-05,"tokenizer_type":"HFTokenizer","data_impl":"mmap","config_files":{"config.yml":"attention-dropout: 0\nbias-gelu-fusion: true\ncheckpoint-activations: true\ncheckpoint-num-layers: 1\ndata-impl: mmap\ndata-path: /mnt/ssd-1/data/pile_20B_tokenizer/pile_20B_tokenizer_text_document\ndistributed-backend: nccl\neval-interval: 1000\neval-iters: 10\nfp16:\n  enabled: true\n  fp16: true\n  hysteresis: 2\n  initial_scale_power: 12\n  loss_scale: 0\n  loss_scale_window: 1000\n  min_loss_scale: 1\ngpt_j_residual: true\ngradient_clipping: 1.0\nhidden-dropout: 0\nhidden-size: 6144\ninit_method: small_init\nload: ./20B_checkpoints\nlog-dir: /mnt/ssd-1/logs\nlog-interval: 2\nlr-decay-iters: 150000\nlr-decay-style: cosine\nmake_vocab_size_divisible_by: 256\nmax-position-embeddings: 2048\nmin_lr: 9.7e-06\nno-weight-tying: true\nno_load_rng: true\nnorm: layernorm\nnum-attention-heads: 64\nnum-layers: 44\noptimizer:\n  params:\n    betas:\n    - 0.9\n    - 0.95\n    eps: 1.0e-08\n    lr: 9.7e-05\n  type: Adam\noutput_layer_init_method: wang_init\noutput_layer_parallelism: column\npartition-activations: false\npipe-parallel-size: 1\npos-emb: rotary\nrotary_pct: 0.25\nsave-interval: 500\nscaled-upper-triang-masked-softmax-fusion: true\nseq-length: 2048\nsplit: 995,4,1\nsteps_per_print: 2\nsynchronize-each-layer: true\ntensorboard-dir: /mnt/ssd-1/tensorboard\ntokenizer_type: HFTokenizer\ntrain-iters: 150000\ntrain_micro_batch_size_per_gpu: 4\nvocab-file: /root/gpt-neox/checkpoints/20B_tokenizer.json\nwall_clock_breakdown: false\nwandb_project: gpt-thicc\nwandb_team: eleutherai\nwarmup: 0.01\nweight-decay: 0.01\nzero_optimization:\n  allgather_bucket_size: 1260000000\n  allgather_partitions: true\n  contiguous_gradients: true\n  cpu_offload: false\n  overlap_comm: true\n  reduce_bucket_size: 1260000000\n  reduce_scatter: true\n  stage: 0\n","text_generation.yml":"# Parameters used for text generation\n# Make sure `load` is specified somewhere else\n{\n  # Text gen type: `input-file`, `unconditional` or `interactive`\n  \"text-gen-type\": \"prompt\",\n\n  # Params for all\n  \"maximum_tokens\": 10,\n  \"temperature\": 1.0,\n  \"top_p\": 0.0,\n  \"top_k\": 0,\n  \"recompute\": false,\n\n  # `unconditional`: samples\n  \"num-samples\": 10,\n\n  # input/output file\n  \"sample-input-file\": \"sample_input.txt\",\n  \"sample-output-file\": \"sample_output.txt\",\n}\n"},"load":"./20B_checkpoints","save_interval":500,"no_load_rng":true,"train_iters":150000,"eval_iters":10,"split":"995,4,1","vocab_file":"/root/gpt-neox/20B_checkpoints/20B_tokenizer.json","attention_dropout":0,"hidden_dropout":0,"checkpoint_activations":true,"synchronize_each_layer":true,"gas":32,"clip_grad":1.0,"dynamic_loss_scale":true,"is_pipe_parallel":true,"log_interval":2,"text_gen_type":"prompt","temperature":1.0,"maximum_tokens":10,"sample_input_file":"sample_input.txt","sample_output_file":"sample_output.txt","num_samples":10,"user_script":"generate.py"}
